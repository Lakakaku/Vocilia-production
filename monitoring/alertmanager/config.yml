# AlertManager Configuration for AI Feedback Platform
# Enhanced for Performance & Load Testing Monitoring
global:
  smtp_smarthost: '${SMTP_HOST:-smtp.gmail.com:587}'
  smtp_from: '${SMTP_FROM:-alerts@feedback.your-domain.com}'
  smtp_auth_username: '${SMTP_USERNAME}'
  smtp_auth_password: '${SMTP_PASSWORD}'
  smtp_require_tls: true

  # Slack configuration
  slack_api_url: '${SLACK_WEBHOOK_URL}'

  # HTTP configuration for webhooks
  http_config:
    tls_config:
      insecure_skip_verify: false

# Alert routing tree - Enhanced for Performance & Load Testing
route:
  # Default receiver for unmatched alerts
  receiver: 'default-team'
  
  # Group alerts by these labels to reduce noise
  group_by: ['alertname', 'cluster', 'service', 'region']
  
  # Time to wait before sending a notification for a group of alerts
  group_wait: 30s
  
  # Time to wait before sending notification about changes to existing group
  group_interval: 5m
  
  # Time to wait before re-sending a notification
  repeat_interval: 4h

  # Routing rules for different types of alerts
  routes:
    # Critical system alerts - immediate notification
    - match:
        severity: critical
      receiver: 'critical-alerts'
      group_wait: 10s
      group_interval: 1m
      repeat_interval: 30m
      continue: true

    # Performance SLA violations - urgent response
    - match_re:
        alertname: 'VoiceProcessingLatencyHigh|APIResponseTimeHigh|LoadTestFailure'
      receiver: 'performance-critical'
      group_wait: 15s
      group_interval: 2m
      repeat_interval: 1h
      routes:
        # Voice processing SLA violations
        - match_re:
            alertname: 'VoiceProcessingLatency.*'
        - receiver: 'voice-sla-team'
          group_by: ['alertname', 'region', 'business_tier']
        
        # API performance SLA violations
        - match_re:
            alertname: 'APIResponseTime.*'
          receiver: 'api-performance-team'
          group_by: ['alertname', 'service', 'endpoint']
        
        # Load testing failures
        - match_re:
            alertname: 'LoadTest.*'
          receiver: 'load-testing-team'
          group_by: ['alertname', 'test_type', 'scenario']

    # High severity alerts - quick notification
    - match:
        severity: high
      receiver: 'high-priority-alerts'
      group_wait: 30s
      group_interval: 2m
      repeat_interval: 1h

    # Voice processing alerts
    - match_re:
        service: 'voice.*|websocket.*'
      receiver: 'voice-team'
      group_by: ['alertname', 'service', 'region']
      group_interval: 3m
      repeat_interval: 2h

    # AI and ML system alerts
    - match_re:
        service: 'ai.*|ml.*|evaluation.*'
      receiver: 'ai-team'
      group_by: ['alertname', 'service', 'model']
      group_interval: 5m
      repeat_interval: 2h

    # Payment processing alerts
    - match_re:
        service: 'payment.*|stripe.*|billing.*'
      receiver: 'payments-team'
      group_by: ['alertname', 'service', 'payment_method']
      group_interval: 2m
      repeat_interval: 1h

    # Fraud detection alerts
    - match_re:
        service: 'fraud.*|security.*'
      receiver: 'security-team'
      group_by: ['alertname', 'service', 'risk_level']
      group_interval: 1m
      repeat_interval: 30m
      continue: true

    # Business KPI alerts - Swedish regions
    - match_re:
        region: 'stockholm|gothenburg|malmo|sweden'
      receiver: 'swedish-operations'
      group_by: ['alertname', 'region', 'business_tier']
      group_interval: 10m
      repeat_interval: 4h

    # Load testing and performance alerts
    - match_re:
        service: 'load.*|performance.*|sla.*'
      receiver: 'performance-team'
      group_by: ['alertname', 'test_type', 'region']
      group_interval: 5m
      repeat_interval: 2h

    # Database alerts
    - match_re:
        service: 'postgres.*|database.*'
      receiver: 'database-team'
      group_by: ['alertname', 'service', 'database']
      group_interval: 3m
      repeat_interval: 1h

    # Infrastructure alerts
    - match_re:
        service: 'node.*|container.*|system.*'
      receiver: 'infrastructure-team'
      group_by: ['alertname', 'service', 'instance']
      group_interval: 5m
      repeat_interval: 2h

    # Warning level alerts - less frequent
    - match:
        severity: warning
      receiver: 'warning-alerts'
      group_interval: 10m
      repeat_interval: 6h

    # Business hours only alerts
    - match:
        severity: low
      receiver: 'business-hours-alerts'
      group_wait: 10m
      group_interval: 1h
      repeat_interval: 24h
      active_time_intervals:
        - business_hours_cet

# Time intervals for business hours alerting
time_intervals:
  - name: business_hours_cet
    time_intervals:
      - times:
          - start_time: '08:00'
            end_time: '18:00'
        weekdays: ['monday:friday']
        location: 'Europe/Stockholm'

# Inhibition rules to prevent alert spam
inhibit_rules:
  # If system is down, don't alert on individual service issues
  - source_matchers:
      - alertname="SystemDowntime"
    target_matchers:
      - component=~"voice|payments|api"
    equal: ['cafe_name', 'environment']

  # If voice processing is down, don't alert on voice latency
  - source_matchers:
      - alertname="VoiceProcessingDown"
    target_matchers:
      - alertname="VoiceLatencyHigh"
    equal: ['cafe_name']

  # If payment processing is failing, don't alert on individual payment issues
  - source_matchers:
      - alertname="PaymentProcessingFailure"
    target_matchers:
      - alertname="StripeWebhookDown"
    equal: ['cafe_name']

# Notification receivers - Enhanced for Performance & Load Testing
receivers:
  # Default team for unclassified alerts
  - name: 'default-team'
    email_configs:
      - to: '${ALERT_EMAIL_DEFAULT:-team@feedback.your-domain.com}'
        subject: '[AI Feedback Alert] {{ .GroupLabels.alertname }}'
        body: |
          {{ range .Alerts }}
          Alert: {{ .Annotations.summary }}
          Description: {{ .Annotations.description }}
          Severity: {{ .Labels.severity }}
          Service: {{ .Labels.service }}
          Instance: {{ .Labels.instance }}
          Value: {{ .Annotations.value }}
          Started: {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#general-alerts'
        title: '{{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Severity:* {{ .Labels.severity }}
          *Service:* {{ .Labels.service }}
          *Started:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}

  # Critical system alerts
  - name: 'critical-alerts'
    email_configs:
      - to: '${ALERT_EMAIL_CRITICAL:-critical-alerts@feedback.your-domain.com,on-call@feedback.your-domain.com}'
        subject: 'üö® CRITICAL ALERT: {{ .GroupLabels.alertname }}'
        body: |
          CRITICAL SYSTEM ALERT DETECTED
          
          {{ range .Alerts }}
          üî• ALERT: {{ .Annotations.summary }}
          üìù Description: {{ .Annotations.description }}
          üè∑Ô∏è Severity: {{ .Labels.severity }}
          üîß Service: {{ .Labels.service }}
          üñ•Ô∏è Instance: {{ .Labels.instance }}
          üìä Value: {{ .Annotations.value }}
          ‚è∞ Started: {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
          üåç Region: {{ .Labels.region }}
          
          ACTION REQUIRED: Immediate investigation needed
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#critical-alerts'
        title: 'üö® Critical Alert: {{ .GroupLabels.alertname }}'
        text: |
          <!channel> Critical issue detected in AI Feedback Platform
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Service:* {{ .Labels.service }}
          *Instance:* {{ .Labels.instance }}
          *Started:* {{ .StartsAt.Format "2006-01-02 15:04:05" }}
          {{ end }}

  # Performance SLA critical alerts
  - name: 'performance-critical'
    email_configs:
      - to: '${ALERT_EMAIL_PERFORMANCE:-performance-critical@feedback.your-domain.com}'
        subject: '‚ö° PERFORMANCE SLA VIOLATION: {{ .GroupLabels.alertname }}'
        body: |
          PERFORMANCE SLA VIOLATION DETECTED
          
          {{ range .Alerts }}
          üéØ SLA Alert: {{ .Annotations.summary }}
          üìù Description: {{ .Annotations.description }}
          üîß Service: {{ .Labels.service }}
          üåç Region: {{ .Labels.region }}
          üìä Current Value: {{ .Annotations.value }}
          üéØ SLA Target: {{ .Annotations.sla_target }}
          ‚è∞ Started: {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
          
          IMMEDIATE ACTION REQUIRED: SLA breach detected
          {{ end }}
    
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#performance-alerts'
        title: '‚ö° Performance SLA Violation'
        text: |
          <!channel> Performance SLA violation detected
          {{ range .Alerts }}
          *Alert:* {{ .Annotations.summary }}
          *Service:* {{ .Labels.service }}
          *Current Value:* {{ .Annotations.value }}
          *SLA Target:* {{ .Annotations.sla_target }}
          {{ end }}

  # Voice processing critical
  - name: 'pilot-voice-critical'
    email_configs:
      - to: 'pilot-voice-team@feedbackai.se'
        subject: 'üéôÔ∏è VOICE CRITICAL: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#voice-processing-alerts'
        title: 'üéôÔ∏è Voice Processing Critical Alert'
    # SMS for voice issues (high customer impact)
    webhook_configs:
      - url: '${SMS_WEBHOOK_URL}'
        send_resolved: false
        http_config:
          basic_auth:
            username: '${SMS_USERNAME}'
            password: '${SMS_PASSWORD}'
        body: |
          {
            "to": "+46701234567",
            "message": "CRITICAL: Voice processing issue in Swedish pilot - {{ .CommonAnnotations.summary }}"
          }

  # Payment processing critical
  - name: 'pilot-payment-critical'
    email_configs:
      - to: 'pilot-payments@feedbackai.se'
        subject: 'üí≥ PAYMENT CRITICAL: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#payment-processing'
        title: 'üí≥ Payment Processing Critical'
    # PagerDuty for payment issues
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_INTEGRATION_KEY}'
        description: 'Swedish Pilot Payment Critical: {{ .CommonAnnotations.summary }}'
        severity: 'critical'
        details:
          component: '{{ .CommonLabels.component }}'
          cafe_name: '{{ .CommonLabels.cafe_name }}'

  # System critical
  - name: 'pilot-system-critical'
    email_configs:
      - to: 'pilot-system-team@feedbackai.se, pilot-manager@feedbackai.se'
        subject: '‚ö†Ô∏è SYSTEM DOWN: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#incident-response'
        title: '‚ö†Ô∏è System Critical Alert'
    # SMS + PagerDuty for system downtime
    webhook_configs:
      - url: '${SMS_WEBHOOK_URL}'
        body: |
          {
            "to": ["+46701234567", "+46701234568"],
            "message": "SYSTEM DOWN: Swedish pilot system critical issue - {{ .CommonAnnotations.summary }}"
          }
    pagerduty_configs:
      - routing_key: '${PAGERDUTY_INTEGRATION_KEY}'
        description: 'Swedish Pilot System Down: {{ .CommonAnnotations.summary }}'
        severity: 'critical'

  # High priority alerts
  - name: 'pilot-high'
    email_configs:
      - to: 'pilot-support@feedbackai.se'
        subject: '‚ö° HIGH: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#pilot-program-alerts'
        title: '‚ö° High Priority Pilot Alert'

  # Business KPI high priority
  - name: 'pilot-business-high'
    email_configs:
      - to: 'pilot-business-team@feedbackai.se, pilot-manager@feedbackai.se'
        subject: 'üìä BUSINESS KPI ALERT: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#pilot-business-metrics'
        title: 'üìä Business KPI Alert'
        text: |
          Business metrics alert for Swedish Pilot Program:
          
          {{ range .Alerts }}
          *Caf√©:* {{ .Labels.cafe_name }}
          *Metric:* {{ .Annotations.summary }}
          *Value:* {{ .Annotations.description }}
          *Action Required:* Review caf√© operations and customer experience
          {{ end }}

  # Fraud detection high priority
  - name: 'pilot-fraud-high'
    email_configs:
      - to: 'pilot-security@feedbackai.se'
        subject: 'üõ°Ô∏è FRAUD ALERT: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#fraud-detection'
        title: 'üõ°Ô∏è Fraud Detection Alert'

  # Medium priority alerts
  - name: 'pilot-medium'
    email_configs:
      - to: 'pilot-support@feedbackai.se'
        subject: '‚ÑπÔ∏è MEDIUM: {{ .CommonAnnotations.summary }}'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#pilot-program-alerts'
        title: '‚ÑπÔ∏è Medium Priority Alert'

  # Business hours only alerts
  - name: 'pilot-business-hours'
    slack_configs:
      - api_url: '${SLACK_WEBHOOK_URL}'
        channel: '#pilot-program-alerts'
        title: '‚ÑπÔ∏è Business Hours Alert'

# Templates for custom alert formatting
templates:
  - '/etc/alertmanager/templates/*.tmpl'